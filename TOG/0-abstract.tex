\begin{abstract}
Streaming-based graphics has revolutionized the entertainment industry. It transfers the computational pressure from individual personal computers to clustered servers. This unifies development/consumption experience and reduces local storage.
Virtual and augmented realities are the future medium that would significantly benefit from the cloud computing via reliving their currently bulky wires, demands for high-end personal computers, and consequently, low resolution/field-of-view. 
However, the current off-the-shelf streaming solutions apply only to console-based platforms due to their lower requirement of latency and quality, thus transmission performance.

Tailored for the future cloud-based virtual reality, we present the first gaze-contingent 3D neural representation and view synthesis method. Our method considers both visual acuity and neural representation for 6DoF immersive interaction. Both objective analysis and subjective studies evidenced our effectiveness of significantly reducing local storage (\warning{xxx MB} for any high quality 3D scene), transmission latency (\warning{xxx ms}), and rendering time (\warning{xxx ms}).
\end{abstract}
