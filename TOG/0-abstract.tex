\begin{abstract}
%Streaming-based graphics has revolutionized the entertainment industry. It transfers the computational pressure from individual personal computers to clustered servers. This unifies development/consumption experience and reduces local storage.
%Virtual and augmented realities are the future medium that would significantly benefit from the cloud computing via reliving their currently bulky wires, demands for high-end personal computers, and consequently, low resolution/field-of-view. 
%However, the current off-the-shelf streaming solutions apply only to console-based platforms due to their lower requirement of latency and quality, thus transmission performance.

Traditional high-quality 3D graphics requires large volume of detailed data to be rendering. This compromises efficiency and local storage. In recent years, cloud-based streaming and neural representations have significantly revolutionized the pipeline by redistributing and compressing the computation/storage. However, they typically suffer from high latency and low quality for practically large virtual scenes with 6 degree-of-freedom camera motions. These problems are particularly exacerbated in immersive medias such as virtual and augmented reality due to the demand of high frequency and resolution. 

Tailored for the future cloud-based, portable, and low-storage VR platforms, we present the first gaze-contingent 3D neural representation and view synthesis approach. Our method mutually bridges both visual acuity and neural representation for 6DoF immersive interaction. Moreoever, it optimizes between temporal computational latency and spatial predictive precision. Both objective analysis and subjective studies evidenced our effectiveness of significantly reducing local storage (\warning{xxx MB, xx\% size reduction} for large 3D scenes) while preserves original perceptual quality without introducing additional latency.
\end{abstract}