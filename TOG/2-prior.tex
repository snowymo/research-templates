\section{Related Work}
\label{sec:prior}

\subsection{Image-based View Synthesis}
Image-based rendering (IBR) has been proposed in computer graphics to complement the traditional 3D assets and ray propagation pipelines \cite{levoy1996light,gortler1996lumigraph,shum2000review}. Benefiting from the uniform 2D representation, it delivers pixel-wise high quality without compromising performance. However, one major limitation of IBR is the demand for large and dense data acquisition, resulting in limited and sparse view ranges. 
To address this problem, recent years have witnessed extensive research on synthesizing novel image-based views instead of fully capturing or storing them. Examples include synthesizing light fields \cite{LearningViewSynthesis,Li2020LF,mildenhall2019llff} and multi-layer volumetric videos \cite{Broxton:immersiveLF}. However, despite being more compact, the synthesis is usually in a locally interpolated fashion than globally applicable with highly dynamic VR camera motions.

\subsection{Implicit Scene Representation for Neural Rendering}
To fully represent a 3D object and environment without large explicit storage, neural representations have drawn extensive attention in computer graphics and computer vision communities. 
With a deep neural network that depicts a 3D world as an implicit function, the neural networks may directly approximate an object's appearance given a camera pose \cite{sitzmann2019deepvoxels,sitzmann2019srns,sitzmann2019siren,mildenhall2020nerf,park2017transformation}. 
Prior arts also investigated implicitly representing shapes ~\cite{park2019deepsdf} and surfaces ~\cite{mescheder2019occupancy} to improve the visual quality of 3D objects. Inspired by \cite{park2019deepsdf}, signed distance functions have been deployed for representing raw data such as point cloud ~\cite{atzmon2020sal,gropp2020implicit} with better efficiency ~\cite{chabra2020deep}. 
The input information ranged from 2D images ~\cite{lin2020sdf, yariv2020multiview,choi2019extreme}, 3D context ~\cite{saito2019pifu, oechsle2019texture, zhang2020deep}, time-varying vector field~\cite{niemeyer2019occupancy} to local features~\cite{tretschk2020patchnets, liu2020neural}.
Current implicit representations primarily focus on locally ``outside-in'' viewing of individual objects, with low field-of-view, speed, and resolution. For large scenes, the lack of coverage may cause the quality to drop.
However, in virtual reality, the cameras are typically first-person and highly dynamic, requiring low latency, high resolution, and 6DoF coverage. 
%Our method, in comparison, is VR tailored via representing the scene with user-centered views.

\subsection{Gaze-Contingent Rendering}
Both rendering and neural synthesis suffer from the heavy computational load, thus systematic latency and lags.
With a full mesh representation, foveated rendering has been proposed to accelerate local rendering performance, including mesh- \cite{Guenter:2012:F3G,Patney:2016:TFR} or image-based \cite{Sun:2017:PGF,Kaplanyan:2019:DNR} methods. The accelerations are typically achieved through high field-of-view displays (such as VR head-mounted displays) and eye-tracking technologies. However, these all require full access to the original 3D assets or per-frame images. Consequently, the systems usually require large local storage, long transmission waits, or constant frame-based transmissions with latency. 
%On the other hand, most view synthesizing methods are designed for offline viewing or processing, other than real-time or interactive consumption, especially with high resolution and FoV.  
Our method bridges human vision and deep synthesis. It accelerates neural synthesis beyond local rendering, enabling both computational and storage efficiency. It is also suitable for future cloud-based virtual reality with instant full-quality viewing and interaction.
%\zh{seems not sufficient?}

\subsection{Panorama-Based 6DoF Immersive Viewing}
Panorama images and videos much fit VR applications thanks to their 360 FoV coverage. However, the main challenge is their single projection center in each frame, limiting free-form camera translation, thus 6DoF immersive viewing experience. 
Recently, extensive research has been proposed to address this problem. Serrano et al. \shortcite{Serrano_TVCG_VR-6dof} presented a depth-based dis-occlusion method that dynamically reprojects to 6DoF cameras, enabling natural viewing with motion parallax. Pozo et al. \shortcite{Pozo:2019:I6V} jointly optimizes capture and viewing processes.
Machine learning approaches have advanced the robustness of various geometric and lighting conditions \cite{Attal:2020:ECCV,Lin:DeepPanorama,Benjamin:2020:RTV}. 
As a spherical image, panoramas are designed for capturing physical worlds with flexible reprojections to displays. The 6DoF viewing typically suffers from trade-offs among performance, achievable resolution, and dis-occlusion effects due to lack of knowledge of the 3D space.
With an orthogonal mission, our model represents and reproduces arbitrary 3D virtual worlds for low storage VR. It lowers memory usage by replacing traditional mesh- or volume-based data than predicting image-space occlusions.